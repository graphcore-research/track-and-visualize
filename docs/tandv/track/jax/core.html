<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>tandv.track.jax.core API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tandv.track.jax.core</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tandv.track.jax.core.track"><code class="name flex">
<span>def <span class="ident">track</span></span>(<span>model_state: Optional[Dict] = None, optimizer_state: Optional[Tuple] = None, track_gradients: bool = False, include: Union[ForwardRef(None), re.Pattern[str], str] = None, exclude: Union[ForwardRef(None), re.Pattern[str], str] = None, stash_value: Optional[Callable[[jax.Array], Any]] = None, async_offload: bool = False, offload_inc: int = 10, offload_type: Literal['.pkl'] = '.pkl', use_wandb: bool = False, init_step=None) ‑> <a title="tandv.track.jax.core.JaxTracker" href="#tandv.track.jax.core.JaxTracker">JaxTracker</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function for initialising the Jax (Flax) Tensor Tracker context manager.</p>
<p>By default it tracks the stastics for
what we refer to as the Activations
(i.e. the outputs of the forward method in the nn module).
However it can also track gradients, weights and optimiser state.
At the end of training it will write all the logs to a LogFrame
(a <code>pd.DataFrame</code> which conforms to the schema outline in our docs)</p>
<pre><code class="language-python">
    # pass whichever pytrees you're tracking into track(...)
    with track(...) as tracker:
        #Your training loop goes here
        for i in range(max_its):
            # pass fwd &amp; bwd pass fn into intercept
            _,..,_ = tracker.intercept(...)
            # pass whichever pytrees you're tracking into step(...)
            tracker.step(...) # at the end of your loop

</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_state</code></strong> :&ensp;<code>Dict | None</code></dt>
<dd>PyTree of the model parameters
(output from <code>your_model.init()</code>)</dd>
<dt><strong><code>optimizer_state</code></strong> :&ensp;<code>Tuple | None</code></dt>
<dd>Tuple containing the optax optimizer
state object(s) (output from <code>your_optimizer.init(model_state)</code>)</dd>
<dt><strong><code>track_gradients</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not you wish to track the gradients.</dd>
<dt>include (None | Pattern[str] | str) : A module or modules (via regex)
you wish to track.</dt>
<dt>exclude (None | Pattern[str] | str) : A module or modules (via regex)
you wish not to track.</dt>
<dt><strong><code>stash_value</code></strong> :&ensp;<code>StashValueFn</code></dt>
<dd>This is the statistics you wish to track,
it defaults to <code>stash_all_stats_and_hist</code>, you can provide a
custom fn here however inspect the other stash_fns to see the
required args/returns values.</dd>
<dt><strong><code>async_offload</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true the set of stashes since last offloaded
are serialised and passed to a seperate python process to be
converted to a Logframe (currently a very limited reduction
in overhead, but working on improving it)</dd>
<dt><strong><code>offload_inc</code></strong> :&ensp;<code>int</code></dt>
<dd>How frequently you wish to the stashes from memory
to disk, i.e. offload more frequently to minimise Torch Tracker's
memory usage. If using wandb, this value should be the same
(or a multiple) of the increment being used to call
`wandb.log'</dd>
<dt>offload_type (Literal['.pkl']): The file format you wish to write the
LogFrame(s) to disk as.</dt>
<dt><strong><code>use_wandb</code></strong> :&ensp;<code>bool</code></dt>
<dd>If you wish to push the Logframe(s) as artifacts
and get summary numerics statistic in wandb.
(<code>offload_inc</code> should be the same (or a multiple) of the
increment being used to call `wandb.log')</dd>
<dt><strong><code>init_step</code></strong> :&ensp;<code>int</code></dt>
<dd>The tracker has an internal step property for
assigning statistics to the correct iteration, if
<code>init_step == None</code>, defaults to zero,
else init_step (if you are continuing
from a checkpoint for example)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>JaxTracker (The context manager)</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tandv.track.jax.core.JaxTracker"><code class="flex name class">
<span>class <span class="ident">JaxTracker</span></span>
<span>(</span><span>stash: Callable[[tandv.track.common._types.Event], tandv.track.common._types.Stash], model_state: Optional[Dict], optimizer_state: Optional[Tuple], track_gradients: bool, async_offload: bool, offload_inc: int, offload_fn: Callable, include: Union[ForwardRef(None), re.Pattern[str], str], exclude: Union[ForwardRef(None), re.Pattern[str], str], use_wandb: bool, name: Optional[str] = None, init_step: int | None = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class JaxTracker(BaseTracker):
    def __init__(
        self,
        stash: Callable[[Event], Stash],
        model_state: Union[Dict, None],
        optimizer_state: Union[Tuple, None],
        track_gradients: bool,
        async_offload: bool,
        offload_inc: int,
        offload_fn: Callable,
        include: Union[Pattern[str], str, None],
        exclude: Union[Pattern[str], str, None],
        use_wandb: bool,
        name: str | None = None,
        init_step: int | None = None,
    ):
        super().__init__(
            stash=stash,
            name=name,
            init_step=init_step,
            async_offload=async_offload,
            offload_inc=offload_inc,
            offload_fn=offload_fn,
            use_wandb=use_wandb,
        )
        self.track_gradients = track_gradients
        self.track_weights = False
        self.track_optimizer = False
        self.include = re.compile(include) if \
            isinstance(include, str) else include
        self.exclude = re.compile(exclude) if \
            isinstance(exclude, str) else exclude

        # Capture statistics on initial states of model and optimiser
        if not isinstance(model_state, NoneType):
            self._stash_model_weights(model_state)
            self.track_weights = True

        if not isinstance(optimizer_state, NoneType):
            self._stash_opt_state(opt_state=optimizer_state)
            self.track_optimizer = True

    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        traceback: Any | None,
    ) -&gt; None:
        return super().__exit__(exc_type, exc, traceback)

    def _check_included(self, name: str) -&gt; bool:
        return ((not self.include) or self.include.search(name)) and \
            not (self.exclude and self.exclude.search(name))  # type: ignore

    def create_event_and_stash(
        self,
        value: Any,
        name: str,
        type: str,
        tensor_type: TT,
        stash=True,
        *args,
        **kwargs,
    ) -&gt; None:

        if stash:
            self.stash_event(
                Event(
                    name=name,
                    type=type,
                    tensor_type=tensor_type,
                    value=value,
                    args=args,
                    kwargs=kwargs,
                )
            )

    def _data_interceptor(self, next_fun, args, kwargs, context):
        &#34;&#34;&#34;
        Captures the output of each linen module for fwd and bwd passes

        Args:
            next_fun: ...
            args : ...
            kwargs: ...
            context: ...

        Returns


        &#34;&#34;&#34;

        output = next_fun(*args, **kwargs)

        name = &#34;.&#34;.join(context.module.path)
        m_type = str(type(context.module))
        # check include and exclude here...
        included = self._check_included(name)

        forward_callback(
            partial(
                self.create_event_and_stash,
                name=name,
                type=m_type,
                tensor_type=&#34;Activation&#34;,
                stash=included,
                args=(),
                kwargs={},
            ),
            output,
        )

        output = backward_callback(
            partial(
                self.create_event_and_stash,
                name=name,
                type=m_type,
                tensor_type=&#34;Gradient&#34;,
                stash=self.track_gradients and included,
                args=(),
                kwargs={},
            ),
            output,
        )

        # assert len(output) == 1
        # backward callback returns a tuple of len 1, where as the \
        # module is expecting what&#39;s in that tuple (index it outß)
        return output[0]

    def _stash_model_weights(self, params: Dict) -&gt; None:
        &#34;&#34;&#34;
        Creates a stash for each weight parameter jax.Array in the network.
        Does not capture stats on bias parameters.

        Args
            params (Dict): PyTree which contains the model state

        &#34;&#34;&#34;
        flat_tree, _ = tree_flatten_with_path(params)
        for path, value in flat_tree:
            if is_weights(path):
                name = &#34;.&#34;.join([pk.key for pk in path[1:-1]])
                if self._check_included(name):
                    self.stash_event(Event(
                        name,
                        None,
                        &#34;Weights&#34;,
                        value, (), {}))

    def _stash_opt_state(self, opt_state: Tuple) -&gt; None:
        &#34;&#34;&#34;
        Creates a stash for each jax.Array containing \
            optimizer state in the network.
        Does not capture stats on opt state w.r.t bias

        Args
            opt_state (Tuple): Tuple of optax state object(s)

        &#34;&#34;&#34;
        for os in opt_state:
            # skip non optstates... or emptystates
            if os.__class__.__orig_bases__[0] is NamedTuple and not isinstance(
                os, EmptyState
            ):
                state_fields: List[str] = [
                    f for f in os._fields if f != &#34;count&#34;]

                for statefield in state_fields:
                    flat_tree, _ = tree_flatten_with_path(
                        getattr(os, statefield))

                    for path, value in flat_tree:
                        if is_weights(path):
                            # check include and exclude here...
                            name = &#34;.&#34;.join([pk.key for pk in path[1:-1]])
                            if self._check_included(name):
                                self.stash_event(
                                    Event(
                                        &#34;.&#34;.join([
                                            pk.key for pk in path[1:-1]]),
                                        None,
                                        f&#34;Optimiser_State\
                                            .{statefield}&#34;,  # type: ignore
                                        value,
                                        (),
                                        {},
                                    )
                                )

    def _stash_gradients(self, grads: Dict) -&gt; None:
        &#34;&#34;&#34;
        Creates a stash for each jax.Array containing \
            weight gradients in the network.
        Does not capture stats on grads w.r.t bias

        Args
            opt_state (Tuple): Tuple of optax state object(s)

        Returns
            None

        &#34;&#34;&#34;
        flat_tree, _ = tree_flatten_with_path(grads)
        for path, value in flat_tree:
            if is_weights(path):
                name = &#34;.&#34;.join([pk.key for pk in path[1:-1]])
                # check include and exclude here...
                if self._check_included(name):
                    self.stash_event(
                        Event(name, None, &#34;Weight_Gradients&#34;, value, (), {})
                    )

    def step(
        self,
        model_state: Optional[Dict] = None,
        optimizer_state: Optional[Tuple] = None,
        gradients: Optional[Dict] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Method to notify the tracker of the completion of a \
            single training step/iteration.
        Takes as optional arguments, the model_state, \
            optimizer_state and gradients to capture stats on them.

        Args
            model_state (Dict | None): PyTree of the model parameters
                Cannot be none if `model_state` arg was provided \
                    to the `track` method when initalisaing the JaxTracker
            optimizer_state (Tuple | None): Tuple containing the \
                optax optimizer state object(s)
                Cannot be none if `model_state` arg was provided to the \
                    `track` method when initalisaing the JaxTracker
            gradients (Dict | None): PyTree of the gradients w.r.t. \
                the model parameters

        Returns
            None

        &#34;&#34;&#34;

        # capture grads
        if self.track_gradients:
            assert not isinstance(
                gradients, NoneType
            ), &#34;You must provide gradients to the step call, as JaxTracker \
                initialised with `track_gradients == True`&#34;
            self._stash_gradients(grads=gradients)
        # clear stashes to global dict

        self._internal_step()

        # capture model_state
        if self.track_weights:
            assert not isinstance(
                model_state, NoneType
            ), &#34;You must provide model_state to the step call, as JaxTracker \
                initialised with `model_state != None`, therefore it is \
                    configured to track stats on parameter tensors&#34;
            self._stash_model_weights(params=model_state)
        # capture opt_state
        if self.track_optimizer:
            assert not isinstance(
                optimizer_state, NoneType
            ), &#34;You must provide `optimizer_state` to the step call, as \
                JaxTracker initialised with `optimizer_state != None`, \
                    therefore it is configured to track stats on the \
                        optimizer state tensors&#34;
            self._stash_opt_state(opt_state=optimizer_state)

    def intercept(self, f: Callable, *args, **kwargs) -&gt; Any:
        &#34;&#34;&#34;
        Higher-order functions which facilitates the capturing \
            of intermediate stats for
        the fwd and bwd passes. Simply executes the \
            function and returns its args.

        Example Usage:
        ```python
            # fn
            def update(model,model_state,opt_state,optimizer, x, y):
                def loss_fn(params, x, y):
                    y_pred = model.apply(params, x)
                    return jnp.mean((y_pred - y) ** 2)
                grads = jax.grad(loss_fn)(model_state,x,y)
                # optional, update step does not need to occur within \
                # context of
                # intercept fn
                updates, opt_state = optimizer.update(\
                    grads, opt_state, model_state)
                model_state = optax.apply_updates(model_state, updates)

                return model_state,opt_state,grads

            # pass the update fn and its args into the trackers intercept fn
            model_state,opt_state,grads = tracker.intercept(\
                update,model,model_state,opt_state,optimizer, x, y)
        ```

        &#34;&#34;&#34;

        with intercept_methods(self._data_interceptor):
            return f(*args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tandv.track.common._tracker.BaseTracker</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="tandv.track.jax.core.JaxTracker.create_event_and_stash"><code class="name flex">
<span>def <span class="ident">create_event_and_stash</span></span>(<span>self, value: Any, name: str, type: str, tensor_type: Literal['Activation', 'Gradient', 'Weights', 'Optimiser_State\\.[a-zA-Z_\\-]+', 'Weight_Gradients'], stash=True, *args, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tandv.track.jax.core.JaxTracker.intercept"><code class="name flex">
<span>def <span class="ident">intercept</span></span>(<span>self, f: Callable, *args, **kwargs) ‑> Any</span>
</code></dt>
<dd>
<div class="desc"><p>Higher-order functions which facilitates the capturing
of intermediate stats for
the fwd and bwd passes. Simply executes the
function and returns its args.</p>
<p>Example Usage:</p>
<pre><code class="language-python">    # fn
    def update(model,model_state,opt_state,optimizer, x, y):
        def loss_fn(params, x, y):
            y_pred = model.apply(params, x)
            return jnp.mean((y_pred - y) ** 2)
        grads = jax.grad(loss_fn)(model_state,x,y)
        # optional, update step does not need to occur within                 # context of
        # intercept fn
        updates, opt_state = optimizer.update(                    grads, opt_state, model_state)
        model_state = optax.apply_updates(model_state, updates)

        return model_state,opt_state,grads

    # pass the update fn and its args into the trackers intercept fn
    model_state,opt_state,grads = tracker.intercept(                update,model,model_state,opt_state,optimizer, x, y)
</code></pre></div>
</dd>
<dt id="tandv.track.jax.core.JaxTracker.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, model_state: Optional[Dict] = None, optimizer_state: Optional[Tuple] = None, gradients: Optional[Dict] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Method to notify the tracker of the completion of a
single training step/iteration.
Takes as optional arguments, the model_state,
optimizer_state and gradients to capture stats on them.</p>
<p>Args
model_state (Dict | None): PyTree of the model parameters
Cannot be none if <code>model_state</code> arg was provided
to the <code><a title="tandv.track.jax.core.track" href="#tandv.track.jax.core.track">track()</a></code> method when initalisaing the JaxTracker
optimizer_state (Tuple | None): Tuple containing the
optax optimizer state object(s)
Cannot be none if <code>model_state</code> arg was provided to the
<code><a title="tandv.track.jax.core.track" href="#tandv.track.jax.core.track">track()</a></code> method when initalisaing the JaxTracker
gradients (Dict | None): PyTree of the gradients w.r.t.
the model parameters</p>
<p>Returns
None</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tandv.track.jax" href="index.html">tandv.track.jax</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tandv.track.jax.core.track" href="#tandv.track.jax.core.track">track</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tandv.track.jax.core.JaxTracker" href="#tandv.track.jax.core.JaxTracker">JaxTracker</a></code></h4>
<ul class="">
<li><code><a title="tandv.track.jax.core.JaxTracker.create_event_and_stash" href="#tandv.track.jax.core.JaxTracker.create_event_and_stash">create_event_and_stash</a></code></li>
<li><code><a title="tandv.track.jax.core.JaxTracker.intercept" href="#tandv.track.jax.core.JaxTracker.intercept">intercept</a></code></li>
<li><code><a title="tandv.track.jax.core.JaxTracker.step" href="#tandv.track.jax.core.JaxTracker.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
