{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../test-data/numerics_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.ioff()\n",
    "from typing import Tuple\n",
    "import seaborn as sns\n",
    "def plot_single_layer_temporal_heat_map(\n",
    "        log: pd.DataFrame, \n",
    "        layer_name: str, \n",
    "        fig_title: str = None, \n",
    "        interval: int =  50, \n",
    "        figsize: Tuple[int,int] = (20,10)):\n",
    "    \"\"\"\n",
    "        nrows: total_its // interval\n",
    "        ncols: Num tensor types you're logging (i.e. gradients, activations, weights, optimiser_state)\n",
    "\n",
    "\n",
    "        TO-DO: \n",
    "            Global colour legend & unify scales\n",
    "    \"\"\"\n",
    "    which_tensor = ('metadata', 'grad') # to index for activations vs. gradients\n",
    "    which_it = ('metadata', 'step')\n",
    "    which_layer = ('metadata', 'name')\n",
    "\n",
    "    assert layer_name in log[which_layer].unique(), f'{layer_name} is not in the logs provided'\n",
    "    assert np.array_equal(log[which_it].unique(), log[log[which_layer] == layer_name][which_it].unique()) == True, f'{layer_name} has not been logged for each training iteration' # perhaps this should be change to ensure all interval are in the steps column\n",
    "\n",
    "    mod_log = log[log[which_layer] == layer_name] # df for layer in question only\n",
    "\n",
    "    final_it = mod_log[which_it].max()\n",
    "    assert final_it % interval == 0, 'Interval must be a multiple of total number of iterations'\n",
    "    n_rows = final_it // interval + 1 # to ensure final step is displayed.\n",
    "    # should probably do some soft warning if figsize[1] / n_rows is below some threshold for certain plots\n",
    "    tensors_logged = mod_log[which_tensor].unique()\n",
    "    # intialise sub plots\n",
    "    fig, axs = plt.subplots(ncols=len(tensors_logged), nrows=n_rows, figsize=figsize, sharex='col', sharey='row')\n",
    "\n",
    "    # loop for generating subplots\\\n",
    "    # iterate over rows\n",
    "    for it,ax in zip(range(0,final_it+1, interval),axs):\n",
    "\n",
    "        mod_it_log = mod_log[mod_log[which_it] == it] #filter df for interval it\n",
    "        # iterate over columns\n",
    "        for j,col in enumerate(ax):\n",
    "            # logic for column titles:\n",
    "            tensor_tbd = tensors_logged[j]\n",
    "            if it == 0:\n",
    "                title = 'Activations'\n",
    "                if tensor_tbd:\n",
    "                    title = 'Gradients'\n",
    "                col.set_title(title)\n",
    "\n",
    "            # draw subplot\n",
    "            mod_it_tensor_log = mod_it_log[mod_it_log[which_tensor] == tensor_tbd]\n",
    "            denom = mod_it_tensor_log.exponent_count.sum(axis =1)\n",
    "            normed_mod_it_tensor_log = mod_it_tensor_log.exponent_count.apply(lambda x: x/denom)\n",
    "\n",
    "            # temp fix to order exponent column\n",
    "            col_list = normed_mod_it_tensor_log.columns.to_list()\n",
    "            col_list = col_list[-2:-1] + col_list[:-2] + col_list[-1:]\n",
    "\n",
    "            # generate some plot -> \n",
    "            sns.heatmap(normed_mod_it_tensor_log[col_list],ax=col)\n",
    "            col.set_yticks([])\n",
    "            col.set_ylabel(f'n={it}')\n",
    "    return fig\n",
    "\n",
    "# plot_single_layer_temporal_heat_map(df,layer_name='tok_embeddings',interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots to implement...\n",
    "    # modules (across layers) with histograms averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =df.query(\"@df.metadata.name.str.contains('.attention.wk$')\").pipe(\n",
    "    lambda x : x[x.metadata.step == 0]).pipe(lambda x: x[x.metadata.grad == False]).groupby(\n",
    "        df.metadata.type).exponent_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "torch.nn.modules.linear.Linear    6193152\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.exponent_count.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
