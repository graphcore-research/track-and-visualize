{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# add local lib to sys path for relative import\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tt\n",
    "from src.log import torch as tt\n",
    "import torch\n",
    "import torch.amp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        # self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(240, 120,dtype=torch.bfloat16)\n",
    "        self.fc2 = nn.Linear(120, 84,dtype=torch.float32)\n",
    "        self.fc3 = nn.Linear(84, 10,dtype=torch.bfloat16)\n",
    "        self.relu = nn.functional.relu\n",
    "\n",
    "        # for name, layer in self._modules.items():\n",
    "        #     print(name)\n",
    "            # layer.register_forward_hook(lambda x,y,z: print(z,'\\n' + '-'*50 + '\\n'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = self.relu(self.fc1(x.bfloat16()))\n",
    "        x = self.relu(self.fc2(x.float()))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        # x = self.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    # def num_flat_features(self, x):\n",
    "    #     size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "    #     num_features = 1\n",
    "    #     for s in size:\n",
    "    #         num_features *= s\n",
    "    #     return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# print(net)\n",
    "Xi = torch.rand(1,240)\n",
    "# print(Xi,\"\\n\")\n",
    "# net(Xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tt.track(net) as tracker:\n",
    "    net(Xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Stash(name='fc1', type=<class 'torch.nn.modules.linear.Linear'>, grad=False, value=tensor([[ 0.2393,  0.7695,  0.1777, -0.0530, -0.0435,  0.5859,  0.4141,  0.6367,\n",
       "          -0.3047, -0.3574, -0.1963, -0.3398,  0.2402, -0.3770,  0.1621,  0.3027,\n",
       "           0.0962, -0.2363, -0.2129, -0.4004, -0.5469,  0.3379, -0.5039,  0.3066,\n",
       "           0.6289,  0.2305, -0.0698,  0.0310,  0.0540,  0.2715, -0.0820,  0.1162,\n",
       "           0.1738, -0.0100,  0.0703, -0.0427,  0.3398, -0.3301, -0.0439, -0.0198,\n",
       "           0.4043,  0.1133, -0.0479, -0.0918, -0.2324,  0.6914, -0.6406,  0.2539,\n",
       "           0.0089,  0.1982,  0.3711, -0.2422, -0.4785, -0.2949,  0.6836,  0.5430,\n",
       "          -0.0835, -0.2754,  0.0894,  0.3516,  0.6484,  0.1963, -0.6133, -0.0459,\n",
       "          -0.2451,  0.4297,  0.1318, -0.5156,  0.1729,  0.1011,  0.2217,  0.1396,\n",
       "           0.1650, -0.2812, -0.2021,  0.7070, -0.0126, -0.0093,  0.1797,  0.5703,\n",
       "          -0.2012,  0.0161,  0.3203, -0.1465,  0.5000, -0.0996,  0.2832, -0.3594,\n",
       "           0.1719, -0.4512,  0.0815, -0.2324, -0.2891, -0.6289,  0.1484, -0.5547,\n",
       "           0.2715, -0.1973, -0.3652, -0.0200, -0.0371,  0.2832, -0.2656, -0.1934,\n",
       "          -0.0175, -0.0376,  0.3027, -0.5156, -0.4570, -0.3613, -0.4023, -0.3711,\n",
       "          -0.0315, -0.4180, -0.1621,  0.2168, -0.1270,  0.4238,  0.6250,  0.5352]],\n",
       "        dtype=torch.bfloat16)),\n",
       " Stash(name='fc2', type=<class 'torch.nn.modules.linear.Linear'>, grad=False, value=tensor([[-0.0359, -0.0047,  0.0785,  0.1460, -0.2062, -0.0474, -0.0635,  0.0356,\n",
       "          -0.0355,  0.1993,  0.0667, -0.1626,  0.1599,  0.1018,  0.0522,  0.1287,\n",
       "           0.0425, -0.0508,  0.0222, -0.1053, -0.0105, -0.1428, -0.1907, -0.0442,\n",
       "          -0.1989, -0.1537, -0.1314, -0.1666, -0.2208, -0.1723, -0.1439,  0.2181,\n",
       "           0.0150,  0.0514,  0.1660,  0.0035,  0.0787, -0.0362, -0.0052,  0.0577,\n",
       "          -0.2176,  0.1025,  0.2063, -0.0042,  0.2631,  0.1254,  0.0697,  0.1019,\n",
       "          -0.2655,  0.2523,  0.1722,  0.2978, -0.1183,  0.0633, -0.2712, -0.2326,\n",
       "          -0.1500, -0.1486,  0.1481,  0.0106, -0.1473, -0.0403,  0.0037, -0.0405,\n",
       "          -0.2403,  0.0010,  0.1367, -0.1385,  0.1726,  0.0147,  0.1487,  0.2199,\n",
       "           0.0149, -0.1578,  0.0818,  0.0837,  0.0645, -0.1408,  0.0332, -0.0241,\n",
       "           0.0464, -0.0827,  0.0295, -0.2010]])),\n",
       " Stash(name='', type=<class '__main__.Net'>, grad=False, value=tensor([[0.0000, 0.0000, 0.0785, 0.1460, 0.0000, 0.0000, 0.0000, 0.0356, 0.0000,\n",
       "          0.1993, 0.0667, 0.0000, 0.1599, 0.1018, 0.0522, 0.1287, 0.0425, 0.0000,\n",
       "          0.0222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.2181, 0.0150, 0.0514, 0.1660, 0.0035,\n",
       "          0.0787, 0.0000, 0.0000, 0.0577, 0.0000, 0.1025, 0.2063, 0.0000, 0.2631,\n",
       "          0.1254, 0.0697, 0.1019, 0.0000, 0.2523, 0.1722, 0.2978, 0.0000, 0.0633,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1481, 0.0106, 0.0000, 0.0000, 0.0037,\n",
       "          0.0000, 0.0000, 0.0010, 0.1367, 0.0000, 0.1726, 0.0147, 0.1487, 0.2199,\n",
       "          0.0149, 0.0000, 0.0818, 0.0837, 0.0645, 0.0000, 0.0332, 0.0000, 0.0464,\n",
       "          0.0000, 0.0295, 0.0000]]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker.stashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Bias (do these need to be tracked???)\n",
    "params = [p for p in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 240])\n",
      "\n",
      "torch.Size([120])\n",
      "\n",
      "torch.Size([84, 120])\n",
      "\n",
      "torch.Size([84])\n",
      "\n",
      "torch.Size([10, 84])\n",
      "\n",
      "torch.Size([10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in params:\n",
    "    print(p.data.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
